{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documenter Process\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from config.secret_keys import OPENAI_API_KEY, TAVILY_API_KEY, POLYGON_API_KEY, USER_AGENT\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
    "os.environ['POLYGON_API_KEY'] = POLYGON_API_KEY\n",
    "os.environ['USER_AGENT'] = USER_AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Setting\n",
    "from pathlib import Path\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define State\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated, List\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "\n",
    "# Define Tool\n",
    "import pandas as pd\n",
    "from langchain.tools import tool\n",
    "import yfinance as yf\n",
    "from polygon import RESTClient\n",
    "import requests\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from tavily import TavilyClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Define Agent\n",
    "from datetime import datetime\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Directory Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(os.getcwd())\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "CHART_DIR = BASE_DIR / 'chart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://www.mk.co.kr/news/stock/11209083', # title : 돌아온 외국인에 코스피 모처럼 ‘활짝’…코스닥 700선 탈환\n",
    "    'https://www.mk.co.kr/news/stock/11209254', # title : 힘 못받는 증시에 밸류업 ETF 두 달째 마이너스 수익률\n",
    "    'https://www.mk.co.kr/news/stock/11209229', # title : 서학개미 한 달간 1조원 샀는데···테슬라 400달러 붕괴\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name = 'rag-chroma',\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "retrieve = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    user_question : str\n",
    "    generation : str\n",
    "    messages : Annotated[List[BaseMessage], add_messages]\n",
    "    analyses : Annotated[List[dict], operator.add]\n",
    "    documents : List[str]\n",
    "    combined_report : str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_client = RESTClient(api_key=os.environ[\"POLYGON_API_KEY\"])\n",
    "tavily_client = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-1. Stock Data Load\n",
    "\n",
    "```python\n",
    "fetch_stock_data.run({\"ticker\" : \"NVDA\",\n",
    "                      \"start_date\" : \"2023-01-01\",\n",
    "                      \"end_date\" : \"2024-01-01\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_stock_data(ticker:str, start_date:str, end_date:str):\n",
    "    \"\"\"\n",
    "    Fetch stock data and compare with competitors.\n",
    "    \"\"\"\n",
    "\n",
    "    stock = yf.download(\n",
    "        ticker, \n",
    "        start=start_date, \n",
    "        end=end_date, \n",
    "        progress=False\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(index = stock.index)\n",
    "    df[f'{ticker}'] = stock['Close']\n",
    "\n",
    "    related_companies = polygon_client.get_related_companies(ticker)\n",
    "    competitors = [company.ticker for company in related_companies]\n",
    "\n",
    "    for company in competitors:\n",
    "        try : \n",
    "            comp_ticker = company.replace('.', '-') if '.' in company else company\n",
    "            comp_data = yf.download(\n",
    "                comp_ticker,\n",
    "                start = start_date,\n",
    "                end=end_date,\n",
    "                progress=False\n",
    "            )\n",
    "\n",
    "            if not comp_data.empty:\n",
    "                df[company] = comp_data['Close']\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    csv_path = DATA_DIR / \"stock_data.csv\"\n",
    "    df.to_csv(csv_path)\n",
    "\n",
    "    return f\"Stockfile saved to {csv_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-2. Financial Data Load\n",
    "\n",
    "```python\n",
    "get_financial_data.run({\"ticker\" : \"NVDA\", \"num_years\" : 2})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_financial_data(ticker: str, num_years: int):\n",
    "    \"\"\"\n",
    "    주어진 티커의 재무 데이터를 가져와 DataFrame으로 저장하고 파일 경로를 반환합니다.\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): 종목 코드\n",
    "    num_years (int): 가져올 연도 수\n",
    "    \n",
    "    Returns:\n",
    "    str: 저장된 파일 경로 메시지\n",
    "    \"\"\"\n",
    "    # Polygon REST API Client 설정\n",
    "    client = polygon_client\n",
    "    \n",
    "    # 최근 n개년간 분기별 재무제표 데이터 수집\n",
    "    financials = []\n",
    "    for f in client.vx.list_stock_financials(ticker):\n",
    "        if \"Q\" in f.fiscal_period and f.end_date >= f\"{pd.Timestamp.now().year - num_years}-01-01\":\n",
    "            financials.append(f)\n",
    "    \n",
    "    # DataFrame 생성할 데이터 목록 초기화\n",
    "    data = {\n",
    "        # 기본 정보\n",
    "        \"Fiscal Year\": [],\n",
    "        \"Fiscal Period\": [],\n",
    "        \"End Date\": [],\n",
    "        \n",
    "        # 손익계산서 항목\n",
    "        \"Revenues\": [],\n",
    "        \"Cost of Revenue\": [],\n",
    "        \"Gross Profit\": [],\n",
    "        \"Operating Income\": [],\n",
    "        \"Operating Expenses\": [],\n",
    "        \"Basic EPS\": [],\n",
    "        \n",
    "        # 재무상태표 항목\n",
    "        \"Total Assets\": [],\n",
    "        \"Current Assets\": [],\n",
    "        \"Fixed Assets\": [],\n",
    "        \"Intangible Assets\": [],\n",
    "        \"Other Current Assets\": [],\n",
    "        \"Other Non-current Assets\": [],\n",
    "        \"Total Liabilities\": [],\n",
    "        \"Current Liabilities\": [],\n",
    "        \"Non-current Liabilities\": [],\n",
    "        \"Long Term Debt\": [],\n",
    "        \"Total Equity\": [],\n",
    "        \"Inventory\": [],\n",
    "        \"Accounts Payable\": [],\n",
    "        \"Other Current Liabilities\": [],\n",
    "        \n",
    "        # 현금흐름표 항목\n",
    "        \"Net Cash Flow\": [],\n",
    "        \"Financing Cash Flow\": [],\n",
    "        \n",
    "        # 포괄손익 항목\n",
    "        \"Comprehensive Income\": [],\n",
    "        \"Other Comprehensive Income\": [],\n",
    "        \n",
    "        # 성장률\n",
    "        \"YoY Growth\": [],\n",
    "        \"QoQ Growth\": [],\n",
    "        \n",
    "        # 재무비율\n",
    "        \"Current Ratio\": [],  # 유동비율\n",
    "        \"Debt to Equity Ratio\": [],  # 부채비율\n",
    "        \"Gross Profit Margin\": [],  # 매출총이익률\n",
    "        \"Operating Margin\": [],  # 영업이익률\n",
    "        \"Asset Turnover\": []  # 총자산회전율\n",
    "    }\n",
    "    \n",
    "    # 각 재무제표 항목에서 지표들을 추출하여 DataFrame 구성\n",
    "    for i, f in enumerate(financials):\n",
    "        # 기본 정보\n",
    "        data[\"Fiscal Year\"].append(f.fiscal_year)\n",
    "        data[\"Fiscal Period\"].append(f.fiscal_period)\n",
    "        data[\"End Date\"].append(f.end_date)\n",
    "\n",
    "        # 재무제표 데이터 접근\n",
    "        bs = f.financials.balance_sheet\n",
    "        is_ = f.financials.income_statement\n",
    "        cf = f.financials.cash_flow_statement\n",
    "        ci = f.financials.comprehensive_income\n",
    "\n",
    "        # 손익계산서 데이터 추출\n",
    "        revenue = getattr(is_.revenues, 'value', None) if hasattr(is_, 'revenues') else None\n",
    "        data[\"Revenues\"].append(revenue)\n",
    "        data[\"Cost of Revenue\"].append(getattr(is_.cost_of_revenue, 'value', None) if hasattr(is_, 'cost_of_revenue') else None)\n",
    "        data[\"Gross Profit\"].append(getattr(is_.gross_profit, 'value', None) if hasattr(is_, 'gross_profit') else None)\n",
    "        data[\"Operating Expenses\"].append(getattr(is_.operating_expenses, 'value', None) if hasattr(is_, 'operating_expenses') else None)\n",
    "        data[\"Operating Income\"].append(getattr(is_.operating_expenses, 'value', None) if hasattr(is_, 'operating_expenses') else None)\n",
    "        data[\"Basic EPS\"].append(getattr(is_.basic_earnings_per_share, 'value', None) if hasattr(is_, 'basic_earnings_per_share') else None)\n",
    "        \n",
    "        # 재무상태표 데이터 추출\n",
    "        data[\"Total Assets\"].append(bs['assets'].value if 'assets' in bs else None)\n",
    "        data[\"Current Assets\"].append(bs['current_assets'].value if 'current_assets' in bs else None)\n",
    "        data[\"Fixed Assets\"].append(bs['fixed_assets'].value if 'fixed_assets' in bs else None)\n",
    "        data[\"Intangible Assets\"].append(bs['intangible_assets'].value if 'intangible_assets' in bs else None)\n",
    "        data[\"Other Current Assets\"].append(bs['other_current_assets'].value if 'other_current_assets' in bs else None)\n",
    "        data[\"Other Non-current Assets\"].append(bs['other_noncurrent_assets'].value if 'other_noncurrent_assets' in bs else None)\n",
    "        data[\"Total Liabilities\"].append(bs['liabilities'].value if 'liabilities' in bs else None)\n",
    "        data[\"Current Liabilities\"].append(bs['current_liabilities'].value if 'current_liabilities' in bs else None)\n",
    "        data[\"Non-current Liabilities\"].append(bs['noncurrent_liabilities'].value if 'noncurrent_liabilities' in bs else None)\n",
    "        data[\"Long Term Debt\"].append(bs['long_term_debt'].value if 'long_term_debt' in bs else None)\n",
    "        data[\"Total Equity\"].append(bs['equity'].value if 'equity' in bs else None)\n",
    "        data[\"Inventory\"].append(bs['inventory'].value if 'inventory' in bs else None)\n",
    "        data[\"Accounts Payable\"].append(bs['accounts_payable'].value if 'accounts_payable' in bs else None)\n",
    "        data[\"Other Current Liabilities\"].append(bs['other_current_liabilities'].value if 'other_current_liabilities' in bs else None)\n",
    "        \n",
    "        # 현금흐름표 데이터 추출\n",
    "        data[\"Net Cash Flow\"].append(getattr(cf.net_cash_flow, 'value', None) if hasattr(cf, 'net_cash_flow') else None)\n",
    "        data[\"Financing Cash Flow\"].append(getattr(cf.net_cash_flow_from_financing_activities, 'value', None) \n",
    "                                        if hasattr(cf, 'net_cash_flow_from_financing_activities') else None)\n",
    "        \n",
    "        # 포괄손익 데이터 추출\n",
    "        if hasattr(ci, 'comprehensive_income_loss'):\n",
    "            comp_income = ci.comprehensive_income_loss.value\n",
    "        else:\n",
    "            comp_income = ci.comprehensive_income.value if hasattr(ci, 'comprehensive_income') else None\n",
    "        data[\"Comprehensive Income\"].append(comp_income)\n",
    "        data[\"Other Comprehensive Income\"].append(getattr(ci.other_comprehensive_income_loss, 'value', None) \n",
    "                                                if hasattr(ci, 'other_comprehensive_income_loss') else None)\n",
    "        \n",
    "        # YoY 성장률 계산 (전년 동기 대비)\n",
    "        if i > 3 and data[\"Revenues\"][i-4] is not None and revenue is not None:\n",
    "            yoy_growth = ((revenue - data[\"Revenues\"][i-4]) / data[\"Revenues\"][i-4]) * 100\n",
    "        else:\n",
    "            yoy_growth = None\n",
    "        data[\"YoY Growth\"].append(yoy_growth)\n",
    "        \n",
    "        # QoQ 성장률 계산 (직전 분기 대비)\n",
    "        if i > 0 and data[\"Revenues\"][i-1] is not None and revenue is not None:\n",
    "            qoq_growth = ((revenue - data[\"Revenues\"][i-1]) / data[\"Revenues\"][i-1]) * 100\n",
    "        else:\n",
    "            qoq_growth = None\n",
    "        data[\"QoQ Growth\"].append(qoq_growth)\n",
    "        \n",
    "        # 재무비율 계산\n",
    "        current_assets = bs['current_assets'].value if 'current_assets' in bs else None\n",
    "        current_liabilities = bs['current_liabilities'].value if 'current_liabilities' in bs else None\n",
    "        total_liabilities = bs['liabilities'].value if 'liabilities' in bs else None\n",
    "        total_equity = bs['equity'].value if 'equity' in bs else None\n",
    "        total_assets = bs['assets'].value if 'assets' in bs else None\n",
    "        gross_profit = getattr(is_.gross_profit, 'value', None) if hasattr(is_, 'gross_profit') else None\n",
    "        operating_income = getattr(is_.operating_expenses, 'value', None) if hasattr(is_, 'operating_expenses') else None\n",
    "        \n",
    "        # 유동비율 계산\n",
    "        current_ratio = (current_assets / current_liabilities * 100) if (current_assets and current_liabilities) else None\n",
    "        data[\"Current Ratio\"].append(current_ratio)\n",
    "        \n",
    "        # 부채비율 계산\n",
    "        debt_equity_ratio = (total_liabilities / total_equity * 100) if (total_liabilities and total_equity) else None\n",
    "        data[\"Debt to Equity Ratio\"].append(debt_equity_ratio)\n",
    "        \n",
    "        # 매출총이익률 계산\n",
    "        gross_margin = (gross_profit / revenue * 100) if (gross_profit and revenue) else None\n",
    "        data[\"Gross Profit Margin\"].append(gross_margin)\n",
    "        \n",
    "        # 영업이익률 계산\n",
    "        operating_margin = (operating_income / revenue * 100) if (operating_income and revenue) else None\n",
    "        data[\"Operating Margin\"].append(operating_margin)\n",
    "        \n",
    "        # 총자산회전율 계산\n",
    "        asset_turnover = (revenue / total_assets) if (revenue and total_assets) else None\n",
    "        data[\"Asset Turnover\"].append(asset_turnover)\n",
    "\n",
    "    # DataFrame 생성 및 정렬\n",
    "    df = pd.DataFrame(data)\n",
    "    df.sort_values(by=\"End Date\", ascending=False, inplace=True)\n",
    "    \n",
    "    # 한글 컬럼명으로 변경\n",
    "    korean_columns = {\n",
    "        \"Fiscal Year\": \"회계연도\",\n",
    "        \"Fiscal Period\": \"회계기간\",\n",
    "        \"End Date\": \"기준일\",\n",
    "        \"Revenues\": \"매출액\",\n",
    "        \"Cost of Revenue\": \"매출원가\",\n",
    "        \"Gross Profit\": \"매출총이익\",\n",
    "        \"Operating Income\": \"영업이익\",\n",
    "        \"Operating Expenses\": \"영업비용\",\n",
    "        \"Basic EPS\": \"기본주당순이익\",\n",
    "        \"Total Assets\": \"총자산\",\n",
    "        \"Current Assets\": \"유동자산\",\n",
    "        \"Fixed Assets\": \"고정자산\",\n",
    "        \"Intangible Assets\": \"무형자산\",\n",
    "        \"Other Current Assets\": \"기타유동자산\",\n",
    "        \"Other Non-current Assets\": \"기타비유동자산\",\n",
    "        \"Total Liabilities\": \"총부채\",\n",
    "        \"Current Liabilities\": \"유동부채\",\n",
    "        \"Non-current Liabilities\": \"비유동부채\",\n",
    "        \"Long Term Debt\": \"장기부채\",\n",
    "        \"Total Equity\": \"총자본\",\n",
    "        \"Inventory\": \"재고자산\",\n",
    "        \"Accounts Payable\": \"매입채무\",\n",
    "        \"Other Current Liabilities\": \"기타유동부채\",\n",
    "        \"Net Cash Flow\": \"순현금흐름\",\n",
    "        \"Financing Cash Flow\": \"재무활동현금흐름\",\n",
    "        \"Comprehensive Income\": \"포괄손익\",\n",
    "        \"Other Comprehensive Income\": \"기타포괄손익\",\n",
    "        \"YoY Growth\": \"전년동기대비성장률\",\n",
    "        \"QoQ Growth\": \"전기대비성장률\",\n",
    "        \"Current Ratio\": \"유동비율\",\n",
    "        \"Debt to Equity Ratio\": \"부채비율\",\n",
    "        \"Gross Profit Margin\": \"매출총이익률\",\n",
    "        \"Operating Margin\": \"영업이익률\",\n",
    "        \"Asset Turnover\": \"총자산회전율\"\n",
    "    }\n",
    "    \n",
    "    df.rename(columns=korean_columns, inplace=True)\n",
    "    \n",
    "    # CSV 파일로 저장\n",
    "    csv_path = DATA_DIR / \"finance_data.csv\"\n",
    "    df.to_csv(csv_path, encoding='utf-8-sig')  # utf-8-sig로 저장하여 한글 깨짐 방지\n",
    "    return f\"재무제표 데이터가 {csv_path}에 저장되었습니다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-3. CSV Data Analize Tool\n",
    "\n",
    "```python\n",
    "analyze_data.run({\"query\" : \"NVDA의 가격 추이에 대해 분석해줘\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_csv_data(query : str):\n",
    "    \"\"\"\n",
    "    저장된 주식 데이터와 재무 데이터를 pandas_agent로 분석하고 질문에 답변합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    custom_prefix = f\"\"\"\n",
    "        You are very smart analyst can use given data.\n",
    "        Please analyze the data in various perspective to fine valuable insight.\n",
    "        You shoould always make the greatest output with accurate metrics and tables.\n",
    "    \"\"\"\n",
    "    \n",
    "    stock_csv_path = DATA_DIR / \"stock_data.csv\"\n",
    "    finance_csv_path = DATA_DIR / \"finance_data.csv\"\n",
    "    upload_data_path = DATA_DIR / \"data.csv\"\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    try : \n",
    "        stock_df = pd.read_csv(stock_csv_path)\n",
    "        df_list.append(stock_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nstock_data path : {stock_csv_path}\\n\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try :\n",
    "        finance_df = pd.read_csv(finance_csv_path)\n",
    "        df_list.append(finance_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nfinance_data path : {finance_csv_path}\\n\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try :\n",
    "        upload_df = pd.read_csv(upload_data_path)\n",
    "        df_list.append(upload_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nuser uploaded data path : {upload_data_path}\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "    pandas_agent = create_pandas_dataframe_agent(\n",
    "        ChatOpenAI(model=\"gpt-4o\"),\n",
    "        df_list,\n",
    "        verbose=True,\n",
    "        agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "        allow_dangerous_code=True,\n",
    "        prefix = custom_prefix # (옵션) prompt에 의도한 문장을 추가\n",
    "    )\n",
    "\n",
    "    result = pandas_agent.run(query)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-4. Chart Generate Tool\n",
    "\n",
    "```python\n",
    "chart_generator.run({\"command\" : \"NVDA stock price chart\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def chart_generator(command:str):\n",
    "    \"\"\"\n",
    "    이 도구는 create_pandas_dataframe_agent를 사용하여 차트를 생성하고 차트를 /charts 폴더에 저장합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    custom_prefix = f\"\"\"\n",
    "        Please make the chart and save in './charts' folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    stock_csv_path = DATA_DIR / \"stock_data.csv\"\n",
    "    finance_csv_path = DATA_DIR / \"finance_data.csv\"\n",
    "    upload_data_path = DATA_DIR / \"data.csv\"\n",
    "    \n",
    "    df_list = []\n",
    "\n",
    "    try : \n",
    "        stock_df = pd.read_csv(stock_csv_path)\n",
    "        df_list.append(stock_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nstock_data path is '{stock_csv_path}'\\n\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try :\n",
    "        finance_df = pd.read_csv(finance_csv_path)\n",
    "        df_list.append(finance_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nfinance_data path is '{finance_csv_path}'\\n\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try :\n",
    "        upload_df = pd.read_csv(upload_data_path)\n",
    "        df_list.append(upload_df)\n",
    "\n",
    "        custom_prefix = custom_prefix + f\"\\nuser uploaded data path is '{upload_data_path}'\"\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    pandas_agent = create_pandas_dataframe_agent(\n",
    "        ChatOpenAI(model=\"gpt-4o\"),\n",
    "        df_list,\n",
    "        verbose=True,\n",
    "        agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "        allow_dangerous_code=True,\n",
    "        prefix = custom_prefix # (옵션) prompt에 의도한 문장을 추가\n",
    "    )\n",
    "\n",
    "    result = pandas_agent.run(command)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-5. Company News Load\n",
    "\n",
    "```python\n",
    "collect_competitor_news.run({\"ticker\":\"NVDA\", \"news_count\":3})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_competitor_news(ticker, news_count):\n",
    "    \"\"\"주어진 티커 관련 회사들의 최신 증권 뉴스를 수집합니다.\n",
    "    Args:\n",
    "        ticker (str): The company ticker.\n",
    "        news_count (int): number how many news will we collect. Basic Number is 10.\n",
    "    Returns:\n",
    "        Dict(List): \n",
    "        A Dict of Competitors' news, each containing recent news articles' description.\n",
    "\n",
    "    Example:\n",
    "        response = competitor_news(\"ticker\":\"AAPL\", \"news_count\":10)\n",
    "        response = {\"MSFT\": [\"US stock....\", \"MS invests on...\", ...]}\n",
    "    \"\"\"\n",
    "    related_companies = polygon_client.get_related_companies(ticker)\n",
    "    competitors = [i.ticker for i in related_companies]\n",
    "    \n",
    "    competitors_news = {}\n",
    "    for c in competitors:\n",
    "        api_key = os.environ[\"POLYGON_API_KEY\"]\n",
    "        api_url = f\"https://api.polygon.io/v2/reference/news?ticker={c}&order=desc&limit={news_count}&sort=published_utc&apiKey={api_key}\"\n",
    "        result = requests.get(api_url).json()\n",
    "        if 'results' in list(result.keys()):\n",
    "            competitors_news[c] = [i['description'] for i in result['results']]\n",
    "    return competitors_news\n",
    "\n",
    "@tool\n",
    "def fetch_company_news(company_name: str) -> str:\n",
    "    \"\"\"Collect recent news for the given company.\"\"\"\n",
    "    search_results = tavily_client.search(query=f\"recent news about {company_name}\", days=7)\n",
    "    return f\"Collected news and market data for {company_name}: \\n{search_results}\"\n",
    "\n",
    "@tool\n",
    "def fetch_market_news(sector: str) -> str:\n",
    "    \"\"\"Collect recent market data for the given company's sector.\"\"\"\n",
    "    search_results = tavily_client.search(query=f\"{sector} industry news\", days=7)\n",
    "    return f\"Collected news and market data for {sector}: {search_results}\"\n",
    "\n",
    "@tool\n",
    "def fetch_webpages_scrape(urls: List[str]) -> str:\n",
    "    \"\"\"Scrape the provided web pages for detailed information.\"\"\"\n",
    "    loader = WebBaseLoader(urls)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\\n\".join(\n",
    "        [f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n",
    "         for doc in docs]\n",
    "    )\n",
    "\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2-6. Non-financial indicators Summary Tool\n",
    "\n",
    "```python\n",
    "get_latest_filing_content({\"ticker\":'NVDA'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fetch_latest_filing_content(ticker: str) -> dict:\n",
    "    \"\"\"주어진 티커에 대한 최신 10-K, 10-Q, 8-K 파일링을 수집하고 해당 파일링의 내용을 추출합니다.\"\"\"\n",
    "    headers = {'User-Agent': \"your.email@example.com\"}\n",
    "    company_tickers_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    response = requests.get(company_tickers_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return f\"Failed to retrieve company tickers.\"\n",
    "    company_tickers = response.json()\n",
    "    company_data = pd.DataFrame.from_dict(company_tickers, orient='index')\n",
    "    company_data['cik_str'] = company_data['cik_str'].astype(str).str.zfill(10)\n",
    "    ticker = ticker.upper()\n",
    "    company_data.set_index('ticker', inplace=True)\n",
    "    if ticker not in company_data.index:\n",
    "        return f\"Ticker {ticker} not found.\"\n",
    "    \n",
    "    cik = company_data.loc[ticker, 'cik_str']\n",
    "    base_url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return f\"Failed to retrieve filings for CIK {cik}\"\n",
    "    data = response.json()\n",
    "    filings = data.get('filings', {}).get('recent', {})\n",
    "    forms = filings.get('form', [])\n",
    "    dates = filings.get('filingDate', [])\n",
    "    accession_numbers = filings.get('accessionNumber', [])\n",
    "    document_urls = filings.get('primaryDocument', [])\n",
    "    df = pd.DataFrame({\n",
    "        'form': forms,\n",
    "        'date': dates,\n",
    "        'accession_number': accession_numbers,\n",
    "        'document_url': document_urls\n",
    "    })\n",
    "    df_filtered = df[df['form'].isin(['10-K', '10-Q', '8-K'])]\n",
    "    latest_filings = df_filtered.sort_values('date', ascending=False).drop_duplicates('form')\n",
    "    results = {}\n",
    "    for _, row in latest_filings.iterrows():\n",
    "        form_type = row['form']\n",
    "        date = row['date']\n",
    "        accession_number = row['accession_number']\n",
    "        document_url = row['document_url']\n",
    "        filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_number.replace('-', '')}/{document_url}\"\n",
    "        response = requests.get(filing_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            results[form_type] = f\"Failed to retrieve the filing from {filing_url}\"\n",
    "        else:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            text_content = soup.get_text(separator='\\n')\n",
    "            results[form_type] = {\n",
    "                'date': date,\n",
    "                'url': filing_url,\n",
    "                'content': text_content\n",
    "            }\n",
    "        # LCEL 체인 구성\n",
    "    def summarize_filings(filings: dict) -> dict:\n",
    "        prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"다음은 {form_type} 파일링의 내용입니다. 주요 재무 지표, 중요한 사실들, \n",
    "            그리고 구체적인 세부 사항을 포함하여 요약해주세요. \n",
    "            최대한 풍부한 요약이 되게끔 해주세요.\n",
    "            \n",
    "            각 보고서 유형에 따라 다음과 같은 중요 정보들을 포함해야 합니다:\n",
    "        \n",
    "            10-K (연간 보고서):\n",
    "            - 주요 재무 지표 (정확한 수치와 함께 매출, 순이익, EPS 등)\n",
    "            - 사업 개요 및 주요 제품/서비스 (구체적인 제품명이나 서비스명 포함)\n",
    "            - 주요 시장 및 고객 (가능한 경우 주요 고객사 이름 포함)\n",
    "            - 경영진의 주요 변동 사항 (해당되는 경우 구체적인 이름과 직책 포함)\n",
    "            - 중요한 위험 요인 (구체적인 예시와 함께)\n",
    "            - 향후 전략 및 전망\n",
    "            - 주요 소송 또는 규제 이슈 (구체적인 사건명이나 관련 기관명 포함)\n",
    "        \n",
    "            10-Q (분기 보고서):\n",
    "            - 분기별 주요 재무 지표 (정확한 수치와 전년 동기 대비 변동률)\n",
    "            - 주요 제품/서비스의 실적 (구체적인 제품명이나 서비스명과 함께)\n",
    "            - 시장 동향 및 경쟁 상황 (가능한 경우 경쟁사 이름 포함)\n",
    "            - 단기적인 위험 요소나 기회 (구체적인 예시와 함께)\n",
    "            - 주요 운영 변경 사항 (해당되는 경우 구체적인 내용 포함)\n",
    "        \n",
    "            8-K (수시 보고서):\n",
    "            - 보고 이벤트의 성격 (예: 경영진 변경, 인수합병, 중요 계약 체결 등)\n",
    "            - 해당 이벤트의 주요 내용 (관련된 모든 당사자의 이름, 금액, 날짜 등 포함)\n",
    "            - 회사에 미치는 잠재적 영향 (가능한 경우 구체적인 수치 예측 포함)\n",
    "            - 관련된 중요 인물의 배경 (해당되는 경우)\n",
    "        \n",
    "            각 항목에 대해 가능한 한 구체적인 세부 사항 (이름, 숫자, 날짜 등)을 포함해주세요. \n",
    "            그러나 전체 요약은 간결해야 하며, 각 항목은 1-3문장으로 제한해주세요.\n",
    "        \n",
    "            파일링 내용:\n",
    "            {text}\n",
    "        \n",
    "            요약:\"\"\"\n",
    "        )\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "        # 배치 처리를 위한 입력 준비\n",
    "        inputs = [\n",
    "            {\"text\": filing_data['content'], \"form_type\": form_type}\n",
    "            for form_type, filing_data in filings.items()\n",
    "            if isinstance(filing_data, dict) and 'content' in filing_data\n",
    "        ]\n",
    "\n",
    "        summaries = chain.batch(inputs)\n",
    "\n",
    "        return {\n",
    "            form_type: summary\n",
    "            for (form_type, filing_data), summary in zip(filings.items(), summaries)\n",
    "            if isinstance(filing_data, dict) and 'content' in filing_data\n",
    "        }\n",
    "\n",
    "    # 파일링 수집 및 요약\n",
    "    summaries = summarize_filings(results)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Define Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-1. Analyst Agent Prompt / Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "today = datetime.today().date().strftime('%Y-%m-%d')\n",
    "\n",
    "financial_prompt = f\"\"\"\n",
    "오늘은 {today}입니다. \n",
    "당신은 뛰어난 재무 분석가입니다. 아래 임무를 지키면서, 데이터를 분석하고, 차트를 생성 및 저장해야 합니다.\n",
    "당신의 임무는:\n",
    "1. 회사의 재무 성명서를 면밀히 분석하여 숨겨진 통찰력을 발견하는 것\n",
    "2. 주요 재무 비율을 계산하고 그 의미를 명확히 해석하는 것\n",
    "3. 수익, 수익성, 현금 흐름의 추세를 파악하고 그에 따른 전략을 제안하는 것\n",
    "4. 회사의 재무 건강 상태와 지속 가능성을 종합적으로 평가하는 것\n",
    "5. 산업 표준과 비교하여 경쟁력을 분석하는 것\n",
    "\n",
    "당신의 추론을 항상 명확히 설명하고, 특정 지표로 결론을 뒷받침하세요.\n",
    "분석을 철저히 하되, 객관성을 유지하는 것을 잊지 마세요.\"\"\"\n",
    "\n",
    "stock_prompt = f\"\"\"오늘은 {today}입니다. \n",
    "당신은 전문 주식 시장 분석가입니다. 아래 임무를 지키면서, 데이터를 분석하고, 차트를 생성 및 저장해야 합니다.\n",
    "당신의 임무는:\n",
    "1. 역사적 주식 가격 움직임을 분석하여 패턴을 식별하는 것\n",
    "2. 기술 지표와 패턴을 평가하여 미래의 가격 변동을 예측하는 것\n",
    "3. 시장 분위기와 모멘텀을 분석하여 투자 기회를 포착하는 것\n",
    "4. 동료 기업과의 가치 평가 지표를 비교하여 상대적 가치를 판단하는 것\n",
    "5. 주요 가격 동인과 촉매를 식별하여 투자 전략을 수립하는 것\n",
    "\n",
    "분석을 지원하는 구체적인 데이터 포인트와 차트를 제공하세요.\n",
    "기술적 및 근본적 요소를 모두 고려하여 평가하세요.\"\"\"\n",
    "\n",
    "market_prompt = f\"\"\"오늘은 {today}입니다. \n",
    "당신은 전문 시장 연구원입니다. 아래 임무를 지키기 위해 SEC 파일링을 탐색하고, 기업, 경쟁사, 시장에 대한 시장 조사를 수행해주세요.\n",
    "당신의 임무는:\n",
    "1. 회사의 경쟁 위치를 분석하여 강점과 약점을 파악하는 것\n",
    "2. 시장 동향과 산업 역학을 평가하여 기회를 탐색하는 것\n",
    "3. 주요 경쟁사와 그들의 전략을 분석하여 시장 내 위치를 이해하는 것\n",
    "4. 시장 기회와 위협을 평가하여 리스크를 관리하는 것\n",
    "5. 회사의 시장 점유율과 성장 가능성을 분석하여 미래 전략을 제안하는 것\n",
    "6. 특정 회사에 관한 분석 내용이 아닐 경우 사용자가 원하는 주제에 대해 검색하여 분석하고 위의 1~5 임무를 수행하는 것\n",
    "\n",
    "결론을 뒷받침하는 구체적인 예시와 데이터를 사용하세요.\n",
    "현재 시장 조건과 미래 동향을 모두 고려하여 분석하세요.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-2. Define ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_agent = create_react_agent(\n",
    "    llm, \n",
    "    [fetch_financial_data, analyze_csv_data, chart_generator], \n",
    "    state_modifier = financial_prompt\n",
    ")\n",
    "stock_agent = create_react_agent(\n",
    "    llm, \n",
    "    [fetch_stock_data, analyze_csv_data, chart_generator], \n",
    "    state_modifier = stock_prompt\n",
    ")\n",
    "market_agent = create_react_agent(\n",
    "    llm, \n",
    "    [fetch_latest_filing_content, fetch_company_news, fetch_competitor_news, fetch_market_news, fetch_webpages_scrape, web_search_tool],\n",
    "    state_modifier = market_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-3-3. define RAG Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary score for relevance check on retrieved documents.\n",
    "    \"\"\"\n",
    "\n",
    "    relevance_score : str = Field(\n",
    "        description=\"Document are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Retreived documents : \\n\\n {documents} \\n\\n User question : {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### inner data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "inner_analyst = write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeHallucination(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary Score for hallunination present in generation answer.\n",
    "    \"\"\"\n",
    "\n",
    "    hallucination_score : str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "structured_llm = llm.with_structured_output(GradeHallucination)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "\n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"Set of facts : \\n\\n {documents} \\n\\n LLM generation : {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerGrader(BaseModel):\n",
    "    \"\"\"\n",
    "    Binary Score to assess answer address question.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_score : str = Field(\n",
    "        description=\"Answer address the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0\n",
    ")\n",
    "structured_llm = llm.with_structured_output(AnswerGrader)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "    \n",
    "    Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\n",
    "\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"User question : \\n\\n {question} \\n\\n LLM generation : {generation}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "    You a question re-writer that converts an input question to a better version that is optimized for vectorstore retrieval. \\n\n",
    "    \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "\"\"\"\n",
    "rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question : \\n\\n {question} \\n Formulation an improved question\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_rewriter = rewrite_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. Define Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "def retrieve_node(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"[Graph Log] RETRIEVE ...\")\n",
    "    user_question = state[\"user_question\"]\n",
    "\n",
    "    documents = retrieve.invoke(user_question)\n",
    "    \n",
    "    return {\n",
    "        \"documents\" : documents,\n",
    "        \"messages\" : messages\n",
    "    }\n",
    "\n",
    "def inner_analyst_node(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"[Graph Log] WRITE ...\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    generation = inner_analyst.invoke({\"context\" : documents, \"question\" : messages})\n",
    "\n",
    "    return {\n",
    "        \"documents\" : documents,\n",
    "        \"messages\" : messages,\n",
    "        \"generation\" : generation\n",
    "    }\n",
    "\n",
    "def filter_documents_node(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args : \n",
    "        state (dict) : The current graph state\n",
    "\n",
    "    Returns : \n",
    "        state (dict) : Updates documents key with only filterd relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"[Graph Log] FILTER DOCUMENTS ...\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\" : question, \"documents\" : doc.page_content}\n",
    "        )\n",
    "        relevance_grade = score.relevance_score\n",
    "\n",
    "        if relevance_grade == \"yes\":\n",
    "            print(\"[Relevance Grader Log] GRADE : DOCUMENT RELEVANT\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"[Relevance Grader Log] GRADE : DOCUMENT NOT RELEVANT\")\n",
    "            continue\n",
    "    \n",
    "    return {\"documents\" : filtered_docs, \"question\" : question}\n",
    "\n",
    "def transform_query_node(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args :\n",
    "        state (dict) : The current graph state\n",
    "    \n",
    "    Returns : \n",
    "        state (dict) : Updateds question key with a re-phrase question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"[Graph Log] TRANSFORM QUERY ...\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    rewrited_query = query_rewriter.invoke({\"question\" : question})\n",
    "\n",
    "    return {\"documents\" : documents, \"question\" : rewrited_query}\n",
    "\n",
    "def web_search_node(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args :\n",
    "        state (dict) : The current graph state\n",
    "\n",
    "    Returns :\n",
    "        state (dict) : Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"[Graph Log] WEB SEARCH ...\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    if isinstance(question, str):\n",
    "        query = question\n",
    "    else:\n",
    "        query = str(question.content) if hasattr(question, 'content') else str(question)\n",
    "\n",
    "    docs = web_search_tool.invoke({\"query\" : query})\n",
    "    web_results = \"\\n\".join([doc[\"content\"] for doc in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\" : documents, \"question\" : question}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finpilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
